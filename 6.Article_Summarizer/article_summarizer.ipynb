{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "92c047e6-67ed-44ee-882b-b7caee276341",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\users\\prash\\appdata\\roaming\\python\\python312\\site-packages (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: sumy in c:\\users\\prash\\appdata\\roaming\\python\\python312\\site-packages (0.11.0)\n",
      "Requirement already satisfied: rake-nltk in c:\\users\\prash\\appdata\\roaming\\python\\python312\\site-packages (1.0.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\prash\\appdata\\roaming\\python\\python312\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prash\\appdata\\roaming\\python\\python312\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prash\\appdata\\roaming\\python\\python312\\site-packages (from requests) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prash\\appdata\\roaming\\python\\python312\\site-packages (from requests) (2026.1.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: docopt<0.7,>=0.6.1 in c:\\users\\prash\\appdata\\roaming\\python\\python312\\site-packages (from sumy) (0.6.2)\n",
      "Requirement already satisfied: breadability>=0.1.20 in c:\\users\\prash\\appdata\\roaming\\python\\python312\\site-packages (from sumy) (0.1.20)\n",
      "Requirement already satisfied: pycountry>=18.2.23 in c:\\users\\prash\\appdata\\roaming\\python\\python312\\site-packages (from sumy) (24.6.1)\n",
      "Requirement already satisfied: nltk>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from sumy) (3.9.1)\n",
      "Requirement already satisfied: chardet in c:\\users\\prash\\appdata\\roaming\\python\\python312\\site-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
      "Requirement already satisfied: lxml>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from breadability>=0.1.20->sumy) (5.2.1)\n",
      "Requirement already satisfied: click in c:\\users\\prash\\appdata\\roaming\\python\\python312\\site-packages (from nltk>=3.0.2->sumy) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\prash\\appdata\\roaming\\python\\python312\\site-packages (from nltk>=3.0.2->sumy) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\prash\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk>=3.0.2->sumy) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\prash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install dependencies\n",
    "\n",
    "!pip install requests beautifulsoup4 sumy rake-nltk\n",
    "import nltk\n",
    "\n",
    "# These are the 3 essentials for your News Summarizer project:\n",
    "nltk.download('punkt')      # For sentence splitting (Sumy)\n",
    "nltk.download('punkt_tab')  # For newer NLTK compatibility (Sumy)\n",
    "nltk.download('stopwords')  # For identifying key phrases (RAKE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e2fc9a1c-a1c9-44cf-a954-aaa669a9b8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from rake_nltk import Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "09724cc9-4ee0-471c-b223-10d144e248de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Data Extraction function \n",
    "\n",
    "def get_only_text(url):\n",
    "    \"\"\"Fetches the title and paragraph text from a URL.\"\"\"\n",
    "    try:\n",
    "        # User-Agent header helps avoid being blocked by servers\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        page = requests.get(url, headers=headers, timeout=10)\n",
    "        \n",
    "        if page.status_code != 200:\n",
    "            return \"Error\", f\"Could not access page. Status code: {page.status_code}\"\n",
    "\n",
    "        soup = BeautifulSoup(page.content, \"lxml\")\n",
    "\n",
    "        # Extract Title safely\n",
    "        title = ' '.join(soup.title.stripped_strings) if soup.title else \"No Title Found\"\n",
    "\n",
    "        # Extract all paragraph text\n",
    "        paragraphs = soup.find_all('p')\n",
    "        text = ' '.join([p.get_text() for p in paragraphs])\n",
    "\n",
    "        return title, text\n",
    "\n",
    "    except Exception as e:\n",
    "        return \"Error\", str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7edbbb04-3c7f-4592-82c8-f74504e0b0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Global markets plunge - Wikinews, the free news source\n",
      "Word Count: 1209\n"
     ]
    }
   ],
   "source": [
    "# Scraping the Article\n",
    "\n",
    "url = \"https://en.wikinews.org/wiki/Global_markets_plunge\"\n",
    "title, article_text = get_only_text(url)\n",
    "\n",
    "print(f\"Title: {title}\")\n",
    "print(f\"Word Count: {len(article_text.split())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "819e0ce2-ff2b-4ea8-8b4e-cf6281acaf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ARTICLE SUMMARY ---\n",
      "The reality is that most investors have been spooked by the sheer pressure that the credit crunch is putting on the global economy.” The Japanese Nikkei 225 has recorded it's third biggest drop in history with a massive sell-off in the exchange that has resulted in USD 250 billion being knocked of the index's value. \"Here's what the American people need to know: that the United States government is acting; we will continue to act to resolve this crisis and restore stability to our markets. “I think we quickly realised that we cannot solve the problems we have got as a result of the sub-prime market collapse simply by improving liquidity,\" he said speaking in Birmingham to business leaders earlier today.\n"
     ]
    }
   ],
   "source": [
    "# Summarization using Sumy(LexRank)\n",
    "\n",
    "def summarize_content(text, sentences_count=5):\n",
    "    # Initialize the parser and tokenizer\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    \n",
    "    # Initialize the LexRank summarizer\n",
    "    summarizer = LexRankSummarizer()\n",
    "    \n",
    "    # Generate the summary\n",
    "    summary = summarizer(parser.document, sentences_count)\n",
    "    \n",
    "    # Combine sentences into a single string\n",
    "    return \" \".join([str(sentence) for sentence in summary])\n",
    "\n",
    "summary_result = summarize_content(article_text, sentences_count=3)\n",
    "print(\"--- ARTICLE SUMMARY ---\")\n",
    "print(summary_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6f934e75-87d2-44b4-bf21-f5b746cb4f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TOP KEYWORDS ---\n",
      "- barclays wealth analyst henk potts commented\n",
      "- oil related companies saw large drops\n",
      "- dow jones industrial average falling\n",
      "- dow jones industrial average fell\n",
      "- ftse 100 index fell dramatically\n",
      "- template {{ editprotected }}\n",
      "- template {{ editprotected }}\n",
      "- creative commons attribution 2\n",
      "- business leaders earlier today\n",
      "- yamato life insurance company\n"
     ]
    }
   ],
   "source": [
    "# Keyword Extraction using Rake\n",
    "\n",
    "def extract_keywords(text):\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(text)\n",
    "    # Get top 10 ranked keyword phrases\n",
    "    return r.get_ranked_phrases()[:10]\n",
    "\n",
    "keywords = extract_keywords(article_text)\n",
    "print(\"--- TOP KEYWORDS ---\")\n",
    "for kw in keywords:\n",
    "    print(f\"- {kw}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
